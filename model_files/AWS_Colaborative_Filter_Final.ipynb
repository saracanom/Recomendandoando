{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1747094742883_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-81-80.ec2.internal:20888/proxy/application_1747094742883_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-85-0.ec2.internal:8042/node/containerlogs/container_1747094742883_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.13.0)\n",
      "Installing collected packages: tzdata, python-dateutil, numpy, pandas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3.9/site-packages, outside environment /mnt1/yarn/usercache/livy/appcache/application_1747094742883_0003/container_1747094742883_0003_01_000001/tmp/spark-95a6ea88-d45e-49e0-8c9d-cf8d4dbd1adb\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "Successfully installed numpy-2.0.2 pandas-2.2.3 python-dateutil-2.9.0.post0 tzdata-2025.2\n",
      "\n",
      "Requirement already satisfied: numpy in ./tmp/spark-95a6ea88-d45e-49e0-8c9d-cf8d4dbd1adb/lib64/python3.9/site-packages (2.0.2)\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 2.23.11 requires python-dateutil<=2.9.0,>=2.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-rPlxec18Izn"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EZLpkFmh8keU"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Movies\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lbe0jZPS8Yak",
    "outputId": "e52d1a3f-074f-4f1f-e8b6-75912ff339bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Movies_filtered loaded in 8.04 s\n",
      " Ratings_filtered loaded in 25.55 s"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 1) Measure load time for movies\n",
    "start = time.perf_counter()\n",
    "movies_filtered = (\n",
    "    spark.read\n",
    "         .option(\"header\",    \"true\")\n",
    "         .option(\"inferSchema\",\"true\")\n",
    "         .csv(\"s3://recomendandoando/processed/movies_filtered/\")\n",
    ")\n",
    "end = time.perf_counter()\n",
    "print(f\" Movies_filtered loaded in {end - start:.2f} s\")\n",
    "\n",
    "# 2) Measure load time for ratings\n",
    "start = time.perf_counter()\n",
    "ratings_filtered = (\n",
    "    spark.read\n",
    "         .option(\"header\",    \"true\")\n",
    "         .option(\"inferSchema\",\"true\")\n",
    "         .csv(\"s3://recomendandoando/processed/ratings_filtered/\")\n",
    ")\n",
    "end = time.perf_counter()\n",
    "print(f\" Ratings_filtered loaded in {end - start:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szpg_lfECsXc"
   },
   "source": [
    "### Select train test sets (77% in the training set, 23% in the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4MvVe4nxFdb",
    "outputId": "26813ed8-c888-4f63-95e3-30f03915bd4f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40189 200948\n",
      "16808 84041"
     ]
    }
   ],
   "source": [
    "total_users = ratings_filtered.select(\"userId\").distinct().count()\n",
    "total_movies = ratings_filtered.select(\"movieId\").distinct().count()\n",
    "percent_users_to_mask = 0.8\n",
    "percent_movies_to_mask = 0.8\n",
    "\n",
    "user_cutoff = int(total_users * (1 - percent_users_to_mask))\n",
    "movie_cutoff = int(total_movies * (1 - percent_movies_to_mask))\n",
    "print(user_cutoff, total_users)\n",
    "print(movie_cutoff, total_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhlE0sVZCweq",
    "outputId": "f4448256-e4d7-4098-cf01-de75006f297d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24579265\n",
      "7322529"
     ]
    }
   ],
   "source": [
    "train_data = ratings_filtered.filter(~((col(\"userId\") > user_cutoff) & (col(\"movieId\") > movie_cutoff)))\n",
    "test_data = ratings_filtered.filter((col(\"userId\") > user_cutoff) & (col(\"movieId\") > movie_cutoff))\n",
    "print(train_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us8MSNsexvtT"
   },
   "source": [
    "## Importar y configurar ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LiYgSIV-xUmD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set a checkpoint directory so checkpoint() will actually write to disk\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n",
    "\n",
    "# cache & materialize train and test sets\n",
    "train_data = train_data.cache()\n",
    "test_data  = test_data.cache()\n",
    "\n",
    "# optional: checkpoint test_data to fully truncate its lineage\n",
    "test_data = test_data.checkpoint()\n",
    "\n",
    "# force the cache/checkpoint to happen now\n",
    "_ = train_data.count()\n",
    "_ = test_data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ThGxjGAr_UBG",
    "outputId": "b3118c54-8746-4a53-812e-27124913eea9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank = 10, RMSE = 0.8430\n",
      "Rank = 20, RMSE = 0.8425\n",
      "Rank = 30, RMSE = 0.8427\n",
      "Best rank: 20 with RMSE = 0.8425\n",
      "Total time for all ranks: 809.98 s"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pow, avg, sqrt\n",
    "\n",
    "# Evaluator for RMSE\n",
    "def rmse_from_df(df, label=\"rating\", pred=\"prediction\"):\n",
    "    mse = df.select(\n",
    "        avg(pow(col(label) - col(pred), 2)).alias(\"mse\")\n",
    "    ).collect()[0][\"mse\"]\n",
    "    return float(mse**0.5)\n",
    "\n",
    "# Try different rank values\n",
    "ranks = [10, 20, 30]\n",
    "results = []\n",
    "\n",
    "start_total = time.perf_counter()\n",
    "\n",
    "for rank in ranks:\n",
    "    als = ALS(\n",
    "        rank=rank,\n",
    "        maxIter=20,\n",
    "        regParam=0.1,\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\",  # drop NaNs from cold-start users/items\n",
    "        nonnegative=True\n",
    "    )\n",
    "\n",
    "    model = als.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    rmse = rmse_from_df(predictions)\n",
    "    results.append((rank, rmse))\n",
    "    print(f\"Rank = {rank}, RMSE = {rmse:.4f}\")\n",
    "    \n",
    "total_elapsed = time.perf_counter() - start_total\n",
    "\n",
    "best_rank, best_rmse = results[0]  # Initialize with the first element\n",
    "\n",
    "for rank, rmse in results[1:]:\n",
    "    if rmse < best_rmse:\n",
    "        best_rank, best_rmse = rank, rmse\n",
    "\n",
    "print(f\"Best rank: {best_rank} with RMSE = {best_rmse:.4f}\")\n",
    "print(f\"Total time for all ranks: {total_elapsed:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model RMSE: 0.8424792920851977\n",
      "Model execution time: 254.93 s"
     ]
    }
   ],
   "source": [
    "# Run best model\n",
    "start_total = time.perf_counter()\n",
    "\n",
    "als = ALS(\n",
    "    rank=20,\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",  # drop NaNs from cold-start users/items\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "best_model = als.fit(train_data)\n",
    "best_predictions = best_model.transform(test_data)\n",
    "best_rmse = rmse_from_df(best_predictions)\n",
    "total_elapsed = time.perf_counter() - start_total\n",
    "print(f\"Best model RMSE: {best_rmse}\")\n",
    "print(f\"Model execution time: {total_elapsed:.2f} s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? Model saved to s3://recomendandoando/models/als_rank20"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "save_path = \"s3://recomendandoando/models/als_rank20\"\n",
    "best_model.write().overwrite().save(save_path)\n",
    "print(f\"✅ Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is how it could be loaded later:\n",
    "# from pyspark.ml.recommendation import ALSModel\n",
    "# loaded_model = ALSModel.load(\"s3://recomendandoando/models/als_rank20\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
